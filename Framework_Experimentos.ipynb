{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEdthVV5njdfd75irc/dpd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarRojasG/Experimentos-GPTValidator/blob/main/Framework_Experimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos GPTValidator"
      ],
      "metadata": {
        "id": "wAu6OvJu7hBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La estructura de carpetas y archivos es la siguiente:\n",
        "\n",
        "* Datasets\n",
        "* Miniprompts\n",
        "  * Ejemplos\n",
        "  * Contexto\n",
        "  * Pregunta\n",
        "  * Respuesta\n",
        "  * Criteria\n",
        "  * Reflection\n",
        "  * Feedback\n",
        "  * Score\n",
        "  * Output\n",
        "* Results\n",
        "\n",
        "La idea es que los resultados de los experimentos sean guardados en archivos después de cada ejecución.\n",
        "\n",
        "Para cada experimento se guardarán los siguientes datos:\n",
        "* Nombre del dataset\n",
        "* Fecha de ejecución\n",
        "* Datos por iteración:\n",
        "  * Puntajes asignados\n",
        "  * Métricas obtenidas (MSE, RMSE, R^2)\n",
        "* Resumen de métricas (Media, Desviación estándar)"
      ],
      "metadata": {
        "id": "MgIgM8i5-UDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos"
      ],
      "metadata": {
        "id": "VKJVfoXg2Qu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 &> /dev/null\n",
        "!pip install openai-multi-client &> /dev/null\n",
        "!git clone https://github.com/rilianx/GPTEvaluator &> /dev/null"
      ],
      "metadata": {
        "id": "Lb7AtYXA7QVg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n Experiments.zip &> /dev/null"
      ],
      "metadata": {
        "id": "xNGUlpj3fBWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, classification_report, r2_score, accuracy_score\n",
        "from GPTEvaluator.GPTEvaluator import chat_gpt_multiple\n",
        "from openai_multi_client import OpenAIMultiClient\n",
        "from datetime import datetime, timedelta\n",
        "from google.colab import userdata\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import openai\n",
        "import getpass\n",
        "import json\n",
        "import re\n",
        "\n",
        "openai.api_key = userdata.get('api_key')\n",
        "\n",
        "# Genera un prompt a partir de los miniprompts especificados en el diccionario data\n",
        "def generate_prompt(data):\n",
        "  def read_file(folder, filename):\n",
        "    path = f\"Experiments/Miniprompts/{folder}/{filename}\"\n",
        "    try:\n",
        "      return open(path, 'r', encoding='utf-8').read()\n",
        "    except:\n",
        "      raise Exception(f\"Error: El archivo {path} no existe\")\n",
        "\n",
        "  prompt = \"\"\n",
        "  n = len(data.items())\n",
        "\n",
        "  for i, (key, value) in enumerate(data.items()):\n",
        "    if key == \"instructions\":\n",
        "      prompt += \"Instructions: \\n\"\n",
        "      for j, (key2, value2) in enumerate(data[key].items()):\n",
        "        miniprompt = read_file(key2, value2)\n",
        "        prompt += miniprompt + \"\\n\\n\"\n",
        "    else:\n",
        "      miniprompt = read_file(key, value)\n",
        "      prompt += miniprompt\n",
        "\n",
        "      if i < n-1:\n",
        "        prompt += \"\\n\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Carga un dataset a partir de un archivo xlsx y valida sus columnas\n",
        "def load_dataset(filename, column_data):\n",
        "  path = f\"Experiments/Datasets/{filename}\"\n",
        "  df = pd.read_excel(path)\n",
        "\n",
        "  mandatory_cols = [\"context\", \"question\", \"answer\", \"real_eval\"]\n",
        "  for key in mandatory_cols:\n",
        "    if key not in column_data.keys():\n",
        "      raise Exception(f\"Error: Debe especificar la columna para la variable {key}\")\n",
        "\n",
        "    value = column_data[key]\n",
        "    if value not in df.columns:\n",
        "      raise Exception(f\"Error: La columna {value} no existe\")\n",
        "\n",
        "    df = df.rename(columns={value: key})\n",
        "\n",
        "  return df\n",
        "\n",
        "# Genera las respuestas con ChatGPT\n",
        "def eval_gpt(df, prompt):\n",
        "  api = OpenAIMultiClient(endpoint=\"chats\", data_template={\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.2, \"n\": 1, \"timeout\":10}, concurrency=50, wait_interval=1, max_retries=3, retry_max=10, retry_multiplier=1)\n",
        "\n",
        "  texts = []\n",
        "  for i, row in df.iterrows():\n",
        "    text = prompt.format(Question=row['question'], Answer=row['answer'], Context=row['context'])\n",
        "    texts.append(text)\n",
        "\n",
        "  answers_gpt = chat_gpt_multiple(api, texts)\n",
        "  return answers_gpt\n",
        "\n",
        "# Calcula los puntajes obtenidos por GPT\n",
        "def get_gpt_scores(answers_gpt, score_function):\n",
        "  # Convierte la respuesta de GPT en un diccionario\n",
        "  def get_gpt_dict(answer_gpt):\n",
        "    pattern = r'\\{[^{}]+\\}'\n",
        "    answer = re.findall(pattern, answer_gpt)[0]\n",
        "    return eval(answer)\n",
        "\n",
        "  gpt_scores = []\n",
        "  for answer in answers_gpt:\n",
        "    try:\n",
        "      gpt_dict = get_gpt_dict(answer[0])\n",
        "    except:\n",
        "      print(f\"Error al extraer diccionario. Respuesta GPT: \\n{answer[0]}\\n\\n\")\n",
        "      gpt_scores.append(None)\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      score = score_function(gpt_dict)\n",
        "      gpt_scores.append(score)\n",
        "    except:\n",
        "      print(f\"Error al calcular puntaje. Respuesta GPT: \\n{answer[0]}\\n\\n\")\n",
        "      gpt_scores.append(None)\n",
        "\n",
        "  return gpt_scores\n",
        "\n",
        "# Obtiene los puntajes reales de un dataset\n",
        "def get_real_scores(df):\n",
        "    return df['real_eval'].tolist()\n",
        "\n",
        "# Calcula las métricas de evaluación\n",
        "def get_stats(real_scores, gpt_scores):\n",
        "  sample_size = len(real_scores)\n",
        "  for i in reversed(range(sample_size)):\n",
        "    if gpt_scores[i] == None:\n",
        "      real_scores.pop(i)\n",
        "      gpt_scores.pop(i)\n",
        "\n",
        "  stats = {\n",
        "      \"confusion_matrix\": confusion_matrix(real_scores, gpt_scores).tolist(),\n",
        "      \"mse\": mean_squared_error(real_scores, gpt_scores),\n",
        "      \"mae\": mean_absolute_error(real_scores, gpt_scores),\n",
        "      \"r2\": r2_score(real_scores, gpt_scores),\n",
        "      \"accuracy\": accuracy_score(real_scores, gpt_scores),\n",
        "      \"sample_size\": len(real_scores)\n",
        "  }\n",
        "  return stats\n",
        "\n",
        "def save_results(dataset, prompt, stats, dataset_size, filepath):\n",
        "  with open(filepath, 'w', encoding='utf-8') as file:\n",
        "    data = {\n",
        "        \"dataset_name\": dataset,\n",
        "        \"dataset_size\": dataset_size,\n",
        "        \"prompt\": prompt,\n",
        "        \"stats\": stats\n",
        "    }\n",
        "    json.dump(data, file, ensure_ascii=False)\n",
        "\n",
        "# Evalúa un prompt y retorna las estadísticas obtenidas\n",
        "def evaluate_prompt(df, prompt, score_function):\n",
        "  try:\n",
        "    answers_gpt = eval_gpt(df, prompt)\n",
        "    real_scores = get_real_scores(df)\n",
        "    gpt_scores = get_gpt_scores(answers_gpt, score_function)\n",
        "    return get_stats(real_scores, gpt_scores)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return\n",
        "\n",
        "# Evalúa un prompt para N repeticiones\n",
        "def evaluate_prompt_n(dataset, column_data, prompt_data, score_function, repetitions):\n",
        "  df = load_dataset(dataset, column_data)\n",
        "  prompt = generate_prompt(prompt_data)\n",
        "  dataset_size = len(df)\n",
        "  stats_list = []\n",
        "\n",
        "  for _ in range(repetitions):\n",
        "    stats = evaluate_prompt(df, prompt, score_function)\n",
        "    stats_list.append(stats)\n",
        "\n",
        "  date = datetime.now() - timedelta(hours=4)\n",
        "  formatted_date = date.strftime('%Y%m%d-%H%M')\n",
        "\n",
        "  filepath = f\"Experiments/Results/{formatted_date}.json\"\n",
        "  save_results(dataset, prompt, stats_list, dataset_size, filepath)\n",
        "\n",
        "# Retorna una lista con la metadata necesaria para generar cada prompt\n",
        "def get_prompt_data_list(prompt_data):\n",
        "\n",
        "\n",
        "\n",
        "# Evalúa varios prompts a la vez con N repeticiones\n",
        "def experiment(dataset, column_data, prompt_data, score_function, repetitions):\n",
        "  prompt_data_list = get_prompt_data_list()\n",
        "  print(prompt_data_list)"
      ],
      "metadata": {
        "id": "gmAq6DONnhJf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data = {\n",
        "    \"examples\": \"examples_1.txt\",\n",
        "    \"context\": \"context_1.txt\",\n",
        "    \"question\": \"question_1.txt\",\n",
        "    \"answer\": \"answer_1.txt\",\n",
        "    \"instructions\": {\n",
        "        \"reflection\": \"reflection_1.txt\",\n",
        "        \"feedback\": \"feedback_1.txt\",\n",
        "        \"score\": \"score_1.txt\",\n",
        "    },\n",
        "    \"criteria\": \"criteria_1.txt\",\n",
        "    \"output\": \"output_1.txt\"\n",
        "}\n",
        "\n",
        "column_data = {\n",
        "    \"context\": \"Contexto\",\n",
        "    \"question\": \"Pregunta\",\n",
        "    \"answer\": \"Respuesta\",\n",
        "    \"real_eval\": \"EvalProfe\"\n",
        "}\n",
        "\n",
        "def score_function(gpt_dict):\n",
        "  score = 0.5 * gpt_dict['correctness'] + 0.3 * gpt_dict['completeness'] + 0.2 * gpt_dict['clarity']\n",
        "  if score <= 2: return 0\n",
        "  elif score <= 5: return 1\n",
        "  elif score <= 8: return 2\n",
        "  return 3\n",
        "\n",
        "evaluate_prompt_n(\"test.xlsx\", column_data, prompt_data, score_function, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeQMKKgTE3nj",
        "outputId": "cd7d1744-4bcb-489d-fb46-61ae52866cc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-1-0-2-0-1-"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_prompt(prompt_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy6xCnTQVoJL",
        "outputId": "a56f98ba-26b4-411c-cd62-20a9ed679cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Examples:**\n",
            "Q: ¿Cómo se podría implementar un historial de navegación web usando dos pilas? El historial debe permitir ir hacia atrás y adelante con las páginas previamente visitadas. Describa un algoritmo.\n",
            "Incorrect Answer: Usamos dos pilas para ir hacia adelante y hacia atrás en el historial.  (Score: 0)\n",
            "\n",
            "Q: ¿Cómo se busca un valor en un árbol rojo-negro? Explique el proceso paso a paso.\n",
            "Incorrect Answer: PAra buscar el valor en un árbol rojo-negro debemos pasar por nodos rojos y negros hasta encontrar el valor. (Score: 0)\n",
            "\n",
            "Q: ¿Por qué el acceso a un elemento específico en un arreglo es O(1), es decir, no depende de la cantidad de datos?\n",
            "Incorrect Answer: El acceso es O(1) por que toma un tiempo constante y no depende de la cantidad de datos. (Score: 0)\n",
            "\n",
            "Q: ¿Cuando se recomienda utilizar arreglos en vez de listas enlazadas? Haga referencia a complejidades temporales en su explicación.\n",
            "Incorrect Answer: Un arreglo es recomendable en determinadas situaciones, mientras que la lista enlazada en otras.\n",
            "Feedback: La respuesta del estudiante es incorrecta ya que no proporciona información nueva y simplemente reformula la pregunta sin agregar profundidad o claridad. (Score: 0)\n",
            "\n",
            "**Context (not visible to students):** {Context}\n",
            "\n",
            "**Question:** {Question}\n",
            "\n",
            "**Student's Answer:** {Answer}\n",
            "\n",
            "Instructions: \n",
            "(analysis) Analyse the \"Student's Answer\".\n",
            "It rewrites the same information provided in the question? or It correctly answers the question providing relevant and deep new information?\n",
            "It is complete, that is, answers all the questions?\n",
            "Use the hidden context as a reference to validate the accuracy and relevance of the student's response.\n",
            "Focus on the alignment between the question asked and the answer provided.\n",
            "\n",
            "(feedback) Provide Feedback to the student considering the analysis. Do not be too strict. It is enough that the student answer correctly and more or less completely the question.\n",
            "Start by stating whether the answer is correct or incorrect, and identify specific areas where the understanding was lacking.\n",
            "If the answer is correct, affirm the student's understanding and potentially add a brief note on why their response was particularly effective or comprehensive.\n",
            "If the answer is incorrect, clearly identify the inaccuracies or errors. Provide specific suggestions on how to improve, ensuring the feedback is constructive.\n",
            "Within 150 words. In Spanish.\n",
            "\n",
            "(score) Assign a score between 0 and 10 to different criteria based on the student's answer and the generated feedback.\n",
            "\n",
            "Criteria are: correctness, completeness, clarity.\n",
            "\n",
            "I expect a dict in python as answer: {{\"analysis\": 'analysis in english, the answer is deep?', \"analysis2\": 'reanalyse the \"Student's Answer\" following the same steps, it is correct the previous analysis? expands', \"feedback\":'very detailed feedback considering previous analysis (in spanish, within 150 words)',\"correctness\":correctness_score,\"completeness\":completeness_score,\"relevance\":relevance_score,\"clarity\":clarity_score}}\n",
            "\n",
            "Python dict:\n"
          ]
        }
      ]
    }
  ]
}