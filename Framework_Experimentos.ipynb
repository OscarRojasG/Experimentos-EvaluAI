{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiBEMcvtvT79fAiC6O1rtU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarRojasG/Experimentos-GPTValidator/blob/main/Framework_Experimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos GPTValidator"
      ],
      "metadata": {
        "id": "wAu6OvJu7hBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La estructura de carpetas y archivos es la siguiente:\n",
        "\n",
        "* Datasets\n",
        "* Prompts\n",
        "  * Ejemplos\n",
        "  * Contexto\n",
        "  * Pregunta\n",
        "  * Respuesta\n",
        "  * Criteria\n",
        "  * Reflection\n",
        "  * Feedback\n",
        "  * Score\n",
        "* Results\n",
        "\n",
        "La idea es que los resultados de los experimentos sean guardados en archivos después de cada ejecución.\n",
        "\n",
        "Para cada experimento se guardarán los siguientes datos:\n",
        "* Nombre del dataset\n",
        "* Fecha de ejecución\n",
        "* Datos por iteración:\n",
        "  * Puntajes asignados\n",
        "  * Métricas obtenidas (MSE, RMSE, R^2)\n",
        "* Resumen de métricas (Media, Desviación estándar)"
      ],
      "metadata": {
        "id": "MgIgM8i5-UDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos"
      ],
      "metadata": {
        "id": "VKJVfoXg2Qu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28 &> /dev/null\n",
        "!pip install openai-multi-client &> /dev/null\n",
        "!git clone https://github.com/rilianx/GPTEvaluator &> /dev/null"
      ],
      "metadata": {
        "id": "Lb7AtYXA7QVg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n Experiments.zip &> /dev/null"
      ],
      "metadata": {
        "id": "xNGUlpj3fBWI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import getpass\n",
        "import pandas as pd\n",
        "import re\n",
        "from GPTEvaluator.GPTEvaluator import chat_gpt_multiple\n",
        "from openai_multi_client import OpenAIMultiClient\n",
        "from google.colab import userdata\n",
        "\n",
        "openai.api_key = userdata.get('api_key')\n",
        "\n",
        "# Genera un prompt a partir de los miniprompts especificados en el diccionario data\n",
        "def generate_prompt(data):\n",
        "  def read_file(folder, filename):\n",
        "    path = f\"Experiments/Miniprompts/{folder}/{filename}\"\n",
        "    try:\n",
        "      return open(path, 'r', encoding='utf-8').read()\n",
        "    except:\n",
        "      raise Exception(f\"Error: El archivo {path} no existe\")\n",
        "\n",
        "  prompt = \"\"\n",
        "  n = len(data.items())\n",
        "\n",
        "  for i, (key, value) in enumerate(data.items()):\n",
        "    if key == \"instructions\":\n",
        "      prompt += \"Instructions: \\n\"\n",
        "      for j, (key2, value2) in enumerate(data[key].items()):\n",
        "        miniprompt = read_file(key2, value2)\n",
        "        prompt += miniprompt + \"\\n\\n\"\n",
        "    else:\n",
        "      miniprompt = read_file(key, value)\n",
        "      prompt += miniprompt\n",
        "\n",
        "      if i < n-1:\n",
        "        prompt += \"\\n\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Carga un dataset a partir de un archivo xlsx y valida sus columnas\n",
        "def load_dataset(filename, column_data):\n",
        "  path = f\"Experiments/Datasets/{filename}\"\n",
        "  df = pd.read_excel(path)\n",
        "\n",
        "  mandatory_cols = [\"context\", \"question\", \"answer\", \"real_eval\"]\n",
        "  for key in mandatory_cols:\n",
        "    if key not in column_data.keys():\n",
        "      raise Exception(f\"Error: Debe especificar la columna para la variable {key}\")\n",
        "\n",
        "    value = column_data[key]\n",
        "    if value not in df.columns:\n",
        "      raise Exception(f\"Error: La columna {value} no existe\")\n",
        "\n",
        "    df = df.rename(columns={value: key})\n",
        "\n",
        "  return df\n",
        "\n",
        "# Genera las respuestas con ChatGPT\n",
        "def eval_gpt(df, prompt):\n",
        "  api = OpenAIMultiClient(endpoint=\"chats\", data_template={\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.2, \"n\": 1, \"timeout\":10}, concurrency=50, wait_interval=1, max_retries=3, retry_max=10, retry_multiplier=1)\n",
        "\n",
        "  texts = []\n",
        "  for i, row in df.iterrows():\n",
        "    text = prompt.format(Question=row['question'], Answer=row['answer'], Context=row['context'])\n",
        "    texts.append(text)\n",
        "\n",
        "  answers_gpt = chat_gpt_multiple(api, texts)\n",
        "  return answers_gpt\n",
        "\n",
        "# Calcula los puntajes obtenidos por GPT\n",
        "def get_gpt_scores(answers_gpt, score_function):\n",
        "  # Convierte la respuesta de GPT en un diccionario\n",
        "  def get_gpt_dict(answer_gpt):\n",
        "    pattern = r'\\{[^{}]+\\}'\n",
        "    answer = re.findall(pattern, answer_gpt)[0]\n",
        "    return eval(answer)\n",
        "\n",
        "  gpt_scores = []\n",
        "  for answer in answers_gpt:\n",
        "    try:\n",
        "      gpt_dict = get_gpt_dict(answer[0])\n",
        "    except:\n",
        "      print(f\"Error al extraer diccionario. Respuesta GPT: \\n{answer[0]}\\n\\n\")\n",
        "      gpt_scores.append(None)\n",
        "      continue\n",
        "\n",
        "    try:\n",
        "      score = score_function(gpt_dict)\n",
        "      gpt_scores.append(score)\n",
        "    except:\n",
        "      print(f\"Error al calcular puntaje. Respuesta GPT: \\n{answer[0]}\\n\\n\")\n",
        "      gpt_scores.append(None)\n",
        "\n",
        "  return gpt_scores\n",
        "\n",
        "# Obtiene los puntajes reales de un dataset\n",
        "def get_real_scores(df):\n",
        "    return df['real_eval'].tolist()\n",
        "\n",
        "# Calcula y muestra las métricas de evaluación\n",
        "def show_stats(real_scores, gpt_scores):\n",
        "  pass\n",
        "\n",
        "# Evalúa un prompt con distintas métricas para un dataset determinado\n",
        "def evaluate_prompt(dataset, column_data, prompt_data, score_function):\n",
        "  try:\n",
        "    df = load_dataset(dataset, column_data)\n",
        "    prompt = generate_prompt(prompt_data)\n",
        "    answers_gpt = eval_gpt(df, prompt)\n",
        "    real_scores = get_real_scores(df)\n",
        "    gpt_scores = get_gpt_scores(answers_gpt, score_function)\n",
        "    show_stats(real_scores, gpt_scores)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "gmAq6DONnhJf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data = {\n",
        "    \"examples\": \"examples_1.txt\",\n",
        "    \"context\": \"context_1.txt\",\n",
        "    \"question\": \"question_1.txt\",\n",
        "    \"answer\": \"answer_1.txt\",\n",
        "    \"instructions\": {\n",
        "        \"reflection\": \"reflection_1.txt\",\n",
        "        \"feedback\": \"feedback_1.txt\",\n",
        "        \"score\": \"score_1.txt\",\n",
        "    },\n",
        "    \"criteria\": \"criteria_1.txt\",\n",
        "    \"output\": \"output_1.txt\"\n",
        "}\n",
        "\n",
        "column_data = {\n",
        "    \"context\": \"Contexto\",\n",
        "    \"question\": \"Pregunta\",\n",
        "    \"answer\": \"Respuesta\",\n",
        "    \"real_eval\": \"EvalProfe\"\n",
        "}\n",
        "\n",
        "def score_function(gpt_dict):\n",
        "  return 0.5 * gpt_dict['correctness'] + 0.3 * gpt_dict['completeness'] + 0.2 * gpt_dict['clarity']\n",
        "\n",
        "evaluate_prompt(\"test.xlsx\", column_data, prompt_data, score_function)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeQMKKgTE3nj",
        "outputId": "ed928e88-a4f8-4e64-9949-8c2fc381dbd6"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-0-1-"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_prompt(prompt_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy6xCnTQVoJL",
        "outputId": "a56f98ba-26b4-411c-cd62-20a9ed679cae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Examples:**\n",
            "Q: ¿Cómo se podría implementar un historial de navegación web usando dos pilas? El historial debe permitir ir hacia atrás y adelante con las páginas previamente visitadas. Describa un algoritmo.\n",
            "Incorrect Answer: Usamos dos pilas para ir hacia adelante y hacia atrás en el historial.  (Score: 0)\n",
            "\n",
            "Q: ¿Cómo se busca un valor en un árbol rojo-negro? Explique el proceso paso a paso.\n",
            "Incorrect Answer: PAra buscar el valor en un árbol rojo-negro debemos pasar por nodos rojos y negros hasta encontrar el valor. (Score: 0)\n",
            "\n",
            "Q: ¿Por qué el acceso a un elemento específico en un arreglo es O(1), es decir, no depende de la cantidad de datos?\n",
            "Incorrect Answer: El acceso es O(1) por que toma un tiempo constante y no depende de la cantidad de datos. (Score: 0)\n",
            "\n",
            "Q: ¿Cuando se recomienda utilizar arreglos en vez de listas enlazadas? Haga referencia a complejidades temporales en su explicación.\n",
            "Incorrect Answer: Un arreglo es recomendable en determinadas situaciones, mientras que la lista enlazada en otras.\n",
            "Feedback: La respuesta del estudiante es incorrecta ya que no proporciona información nueva y simplemente reformula la pregunta sin agregar profundidad o claridad. (Score: 0)\n",
            "\n",
            "**Context (not visible to students):** {Context}\n",
            "\n",
            "**Question:** {Question}\n",
            "\n",
            "**Student's Answer:** {Answer}\n",
            "\n",
            "Instructions: \n",
            "(analysis) Analyse the \"Student's Answer\".\n",
            "It rewrites the same information provided in the question? or It correctly answers the question providing relevant and deep new information?\n",
            "It is complete, that is, answers all the questions?\n",
            "Use the hidden context as a reference to validate the accuracy and relevance of the student's response.\n",
            "Focus on the alignment between the question asked and the answer provided.\n",
            "\n",
            "(feedback) Provide Feedback to the student considering the analysis. Do not be too strict. It is enough that the student answer correctly and more or less completely the question.\n",
            "Start by stating whether the answer is correct or incorrect, and identify specific areas where the understanding was lacking.\n",
            "If the answer is correct, affirm the student's understanding and potentially add a brief note on why their response was particularly effective or comprehensive.\n",
            "If the answer is incorrect, clearly identify the inaccuracies or errors. Provide specific suggestions on how to improve, ensuring the feedback is constructive.\n",
            "Within 150 words. In Spanish.\n",
            "\n",
            "(score) Assign a score between 0 and 10 to different criteria based on the student's answer and the generated feedback.\n",
            "\n",
            "Criteria are: correctness, completeness, clarity.\n",
            "\n",
            "I expect a dict in python as answer: {{\"analysis\": 'analysis in english, the answer is deep?', \"analysis2\": 'reanalyse the \"Student's Answer\" following the same steps, it is correct the previous analysis? expands', \"feedback\":'very detailed feedback considering previous analysis (in spanish, within 150 words)',\"correctness\":correctness_score,\"completeness\":completeness_score,\"relevance\":relevance_score,\"clarity\":clarity_score}}\n",
            "\n",
            "Python dict:\n"
          ]
        }
      ]
    }
  ]
}