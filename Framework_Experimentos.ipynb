{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQqrmE9lZiE2RpEg1iUaUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OscarRojasG/Experimentos-GPTValidator/blob/main/Framework_Experimentos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos GPTValidator"
      ],
      "metadata": {
        "id": "wAu6OvJu7hBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La estructura de carpetas y archivos es la siguiente:\n",
        "\n",
        "* Datasets\n",
        "* Prompts\n",
        "  * Ejemplos\n",
        "  * Contexto\n",
        "  * Pregunta\n",
        "  * Respuesta\n",
        "  * Criteria\n",
        "  * Reflection\n",
        "  * Feedback\n",
        "  * Score\n",
        "* Results\n",
        "\n",
        "La idea es que los resultados de los experimentos sean guardados en archivos después de cada ejecución.\n",
        "\n",
        "Para cada experimento se guardarán los siguientes datos:\n",
        "* Nombre del dataset\n",
        "* Fecha de ejecución\n",
        "* Datos por iteración:\n",
        "  * Puntajes asignados\n",
        "  * Métricas obtenidas (MSE, RMSE, R^2)\n",
        "* Resumen de métricas (Media, Desviación estándar)"
      ],
      "metadata": {
        "id": "MgIgM8i5-UDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Framework para experimentos"
      ],
      "metadata": {
        "id": "VKJVfoXg2Qu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Genera un prompt a partir de los miniprompts especificados en el diccionario data\n",
        "def generate_prompt(data):\n",
        "  def read_file(folder, filename):\n",
        "    path = f\"Miniprompts/{folder}/{filename}\"\n",
        "    try:\n",
        "      return open(path, 'r', encoding='utf-8').read()\n",
        "    except:\n",
        "      raise Exception(f\"Error: El archivo {path} no existe\")\n",
        "\n",
        "  prompt = \"\"\n",
        "  n = len(data.items())\n",
        "\n",
        "  for i, (key, value) in enumerate(data.items()):\n",
        "    if key == \"instructions\":\n",
        "      prompt += \"Instructions: \\n\"\n",
        "      for j, (key2, value2) in enumerate(data[key].items()):\n",
        "        miniprompt = read_file(key2, value2)\n",
        "        prompt += miniprompt + \"\\n\\n\"\n",
        "    else:\n",
        "      miniprompt = read_file(key, value)\n",
        "      prompt += miniprompt\n",
        "\n",
        "      if i < n-1:\n",
        "        prompt += \"\\n\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "# Carga un dataset a partir de un archivo xlsx y valida sus columnas\n",
        "def load_dataset(filename, column_data):\n",
        "  path = f\"Datasets/{filename}\"\n",
        "  df = pd.read_excel(path)\n",
        "\n",
        "  mandatory_cols = [\"context\", \"question\", \"answer\", \"real_eval\"]\n",
        "  for key in mandatory_cols:\n",
        "    if key not in column_data.keys():\n",
        "      raise Exception(f\"Error: Debe especificar la columna para la variable {key}\")\n",
        "\n",
        "    value = column_data[key]\n",
        "    if value not in df.columns:\n",
        "      raise Exception(f\"Error: La columna {value} no existe\")\n",
        "\n",
        "    df = df.rename(columns={value: key})\n",
        "\n",
        "  return df\n",
        "\n",
        "# Realiza un experimento para un prompt y dataset determinados\n",
        "def evaluate_prompt(dataset, column_data, prompt_data):\n",
        "  try:\n",
        "    df = load_dataset(dataset, column_data)\n",
        "    prompt = generate_prompt(prompt_data)\n",
        "    print(prompt)\n",
        "  except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "gmAq6DONnhJf"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_data = {\n",
        "    \"examples\": \"examples_1.txt\",\n",
        "    \"context\": \"context_1.txt\",\n",
        "    \"question\": \"question_1.txt\",\n",
        "    \"answer\": \"answer_1.txt\",\n",
        "    \"instructions\": {\n",
        "        \"reflection\": \"reflection_1.txt\",\n",
        "        \"feedback\": \"feedback_1.txt\",\n",
        "        \"score\": \"score_1.txt\",\n",
        "    },\n",
        "    \"criteria\": \"criteria_1.txt\",\n",
        "    \"output\": \"output_1.txt\"\n",
        "}\n",
        "\n",
        "column_data = {\n",
        "    \"context\": \"Contexto\",\n",
        "    \"question\": \"Pregunta\",\n",
        "    \"answer\": \"Respuesta\",\n",
        "    \"real_eval\": \"EvalProfe\"\n",
        "}\n",
        "\n",
        "evaluate_prompt(\"control2_2024-1.xlsx\", column_data, prompt_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeQMKKgTE3nj",
        "outputId": "82bd3a33-76a8-4c1f-8061-13879c6a3cb0"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Examples:**\n",
            "Q: ¿Cómo se podría implementar un historial de navegación web usando dos pilas? El historial debe permitir ir hacia atrás y adelante con las páginas previamente visitadas. Describa un algoritmo.\n",
            "Incorrect Answer: Usamos dos pilas para ir hacia adelante y hacia atrás en el historial.  (Score: 0)\n",
            "\n",
            "Q: ¿Cómo se busca un valor en un árbol rojo-negro? Explique el proceso paso a paso.\n",
            "Incorrect Answer: PAra buscar el valor en un árbol rojo-negro debemos pasar por nodos rojos y negros hasta encontrar el valor. (Score: 0)\n",
            "\n",
            "Q: ¿Por qué el acceso a un elemento específico en un arreglo es O(1), es decir, no depende de la cantidad de datos?\n",
            "Incorrect Answer: El acceso es O(1) por que toma un tiempo constante y no depende de la cantidad de datos. (Score: 0)\n",
            "\n",
            "Q: ¿Cuando se recomienda utilizar arreglos en vez de listas enlazadas? Haga referencia a complejidades temporales en su explicación.\n",
            "Incorrect Answer: Un arreglo es recomendable en determinadas situaciones, mientras que la lista enlazada en otras.\n",
            "Feedback: La respuesta del estudiante es incorrecta ya que no proporciona información nueva y simplemente reformula la pregunta sin agregar profundidad o claridad. (Score: 0)\n",
            "\n",
            "**Context (not visible to students):** {Context}\n",
            "\n",
            "**Question:** {Question}\n",
            "\n",
            "**Student's Answer:** {Answer}\n",
            "\n",
            "Instructions: \n",
            "(analysis) Analyse the \"Student's Answer\".\n",
            "It rewrites the same information provided in the question? or It correctly answers the question providing relevant and deep new information?\n",
            "It is complete, that is, answers all the questions?\n",
            "Use the hidden context as a reference to validate the accuracy and relevance of the student's response.\n",
            "Focus on the alignment between the question asked and the answer provided.\n",
            "\n",
            "(feedback) Provide Feedback to the student considering the analysis. Do not be too strict. It is enough that the student answer correctly and more or less completely the question.\n",
            "Start by stating whether the answer is excelent, good, poor or unsatisfactory (related to correctness and completeness), and identify specific areas where the understanding or details were lacking.\n",
            "If the answer is excelent or good, affirm the student's understanding and potentially add a brief note on why their response was particularly effective or comprehensive.\n",
            "If the answer is poor or unsatisfactory, clearly identify the inaccuracies or errors. Provide specific suggestions on how to improve, ensuring the feedback is constructive.\n",
            "Within 150 words. In Spanish.\n",
            "\n",
            "(score) Assign a score between 0 and 10 to different criteria based on the student's answer and the generated feedback.\n",
            "\n",
            "Criteria are: correctness, completeness, clarity.\n",
            "\n",
            "I expect a dict in python as answer: {{\"analysis\": 'analysis in english, the answer is deep?', \"analysis2\": 'reanalyse the \"Student's Answer\" following the same steps, it is correct the previous analysis? expands', \"feedback\":'very detailed feedback considering previous analysis (in spanish, within 150 words)',\"correctness\":correctness_score,\"completeness\":completeness_score,\"relevance\":relevance_score,\"clarity\":clarity_score}}\n",
            "\n",
            "Python dict:\n"
          ]
        }
      ]
    }
  ]
}